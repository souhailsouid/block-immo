const { S3Client, PutObjectCommand } = require("@aws-sdk/client-s3");
const { Upload } = require("@aws-sdk/lib-storage");
const { DynamoDBClient } = require("@aws-sdk/client-dynamodb");
const { DynamoDBDocumentClient, UpdateCommand } = require("@aws-sdk/lib-dynamodb");
const { createReadStream } = require("fs");
const { pipeline } = require("stream/promises");
const busboy = require("busboy");
const { v4: uuidv4 } = require("uuid");
const { responses, success } = require("../utils/response");
const { requireAuth } = require("../utils/auth");

const s3 = new S3Client({ 
  region: process.env.AWS_REGION || "eu-west-3"
});

const client = new DynamoDBClient({ region: process.env.AWS_REGION || "eu-west-3" });
const docClient = DynamoDBDocumentClient.from(client);

const MAX_FILE_SIZE = 10 * 1024 * 1024; // 10MB
const ALLOWED_MIME_TYPES = [
  "image/jpeg",
  "image/png",
  "image/webp",
  "image/gif"
];

module.exports.handler = async (event) => {
  try {


    // 1. Validate input
    const propertyId = event.pathParameters?.id;
    if (!propertyId) return responses.badRequest("Missing property ID");
    
    const contentType = event.headers["content-type"] || event.headers["Content-Type"];
    if (!contentType?.includes("multipart/form-data")) {
      return responses.badRequest("Invalid content type");
    }

    // üîê AUTHENTIFICATION ET V√âRIFICATION DES R√îLES
    const devMode = event.queryStringParameters?.devMode === 'true';
    let userId = "anonymous";
    let userEmail = "dev@localhost";

    if (!devMode) {
      const auth = await requireAuth(event);
      if (!auth.success) return responses.unauthorized();
      
      userId = auth.user.userId;
      userEmail = auth.user.email;
      
      // üîê V√âRIFICATION DES R√îLES - Utiliser les groupes du token JWT
      const userGroups = auth.user.groups || [];
      
      // M√™me logique que verify-roles-secure.js : canUploadFiles pour PROFESSIONAL || ADMIN
      const canUpload = userGroups.includes('professional') || userGroups.includes('admin');
      
      if (!canUpload) {
        return responses.unauthorized("Acc√®s r√©serv√© aux professionnels et administrateurs uniquement");
      }
      
      
    }

    // 2. Process multipart data
    const parser = busboy({
      headers: { "content-type": contentType },
      limits: { fileSize: MAX_FILE_SIZE }
    });

    let fileStream;
    let fileInfo = {};
    let existingPhotosJson = null;

    parser.on("file", (fieldname, stream, info) => {
      if (fieldname === "file") {
        fileInfo = {
          filename: info.filename,
          mimeType: info.mimeType,
          encoding: info.encoding
        };
        fileStream = stream;
      } else if (fieldname === "existingPhotos") {
        // Les photos existantes √† conserver
        stream.on("data", (chunk) => {
          try {
            existingPhotosJson = JSON.parse(chunk.toString());
          } catch (e) {
            console.warn("‚ö†Ô∏è Could not parse existing photos");
          }
        });
      } else if (fieldname === "photosToDelete") {
        // Les photos √† supprimer de S3
        stream.on("data", (chunk) => {
          try {
            const photosToDelete = JSON.parse(chunk.toString());
            // Stocker pour suppression apr√®s upload
            event.photosToDelete = photosToDelete;
          } catch (e) {
            console.warn("‚ö†Ô∏è Could not parse photos to delete");
          }
        });
      }
    });

    // 3. Handle stream errors
    parser.on("error", (err) => {
      console.error("Busboy error:", err);
      throw new Error("File processing failed");
    });

    // 4. Start processing
    const bodyBuffer = event.isBase64Encoded
      ? Buffer.from(event.body, "base64")
      : Buffer.from(event.body, "binary");
    
    parser.write(bodyBuffer);
    parser.end();

    // 5. Process new file (if any)
    let newPhotoUrl = null;
    if (fileStream && fileInfo.mimeType) {
      // Validate file metadata
      if (!ALLOWED_MIME_TYPES.includes(fileInfo.mimeType)) {
        return responses.badRequest("Unsupported file type");
      }

      // Generate unique filename
      const extension = fileInfo.filename.split(".").pop() || "bin";
      const s3Key = `property-photos/${propertyId}/${uuidv4()}.${extension}`;

      // Stream directly to S3
      const upload = new Upload({
        client: s3,
        params: {
          Bucket: process.env.S3_BUCKET_NAME,
          Key: s3Key,
          Body: fileStream,
          ContentType: fileInfo.mimeType,
          Metadata: {
            propertyId,
            originalName: fileInfo.filename,
            uploadedBy: userEmail
          }
        },
        partSize: 5 * 1024 * 1024 // 5MB chunks
      });

       await upload.done();


      // Generate URL
      newPhotoUrl = `https://${process.env.S3_BUCKET_NAME}.s3.${process.env.AWS_REGION || "eu-west-3"}.amazonaws.com/${s3Key}`;
    }

    // 6. SUPPRIMER LES PHOTOS DE S3 SI N√âCESSAIRE
    let deletedPhotos = [];
    if (event.photosToDelete && Array.isArray(event.photosToDelete)) {
      
      const { DeleteObjectCommand } = require("@aws-sdk/client-s3");
      
      for (const photoUrl of event.photosToDelete) {
        try {
          // Extraire la cl√© S3 depuis l'URL
          const s3Key = photoUrl.replace(`https://${process.env.S3_BUCKET_NAME}.s3.${process.env.AWS_REGION || "eu-west-3"}.amazonaws.com/`, '');
          
          // V√©rifier que la photo appartient √† cette propri√©t√©
          if (s3Key.includes(`property-photos/${propertyId}/`)) {
            const deleteCommand = new DeleteObjectCommand({
              Bucket: process.env.S3_BUCKET_NAME,
              Key: s3Key
            });
            
            await s3.send(deleteCommand);
            deletedPhotos.push(photoUrl);
          } else {
          }
        } catch (error) {
          console.error("‚ùå Failed to delete photo:", photoUrl, error.message);
        }
      }
    }

    // 7. Build final photo list
    let finalPhotoUrls = [];
    
    // Add existing photos (if any)
    if (existingPhotosJson && Array.isArray(existingPhotosJson)) {
      finalPhotoUrls = existingPhotosJson.filter(url => 
        typeof url === 'string' && url.startsWith('https://') && url.includes('block-immo-images')
      );
    }
    
    // Add new photo (if any)
    if (newPhotoUrl) {
      finalPhotoUrls.push(newPhotoUrl);
    }


    // 7. Update DynamoDB with final list (TOUJOURS, m√™me sans nouvelle photo)
    
    const updateCommand = new UpdateCommand({
      TableName: process.env.DYNAMODB_TABLE,
      Key: {
        PK: `PROPERTY#${propertyId}`,
        SK: `PROPERTY#${propertyId}`
      },
      UpdateExpression: "SET photos = :photos, updatedAt = :updatedAt",
      ExpressionAttributeValues: {
        ":photos": finalPhotoUrls,
        ":updatedAt": new Date().toISOString()
      },
      ReturnValues: "ALL_NEW"
    });

    await docClient.send(updateCommand);
    

    return success(200, {
      success: true,
      message: `Successfully updated property photos (${finalPhotoUrls.length} total)`,
      data: {
        propertyId,
        photos: finalPhotoUrls,
        count: finalPhotoUrls.length,
        newPhotoUrl,
        deletedPhotos,
        deletedCount: deletedPhotos.length,
        updatedAt: new Date().toISOString(),
        updatedBy: userEmail,
        property: result.Attributes
      }
    });

  } catch (error) {
    console.error("‚ùå Upload failed:", error);
    return responses.serverError("File processing error: " + error.message);
  }
};